{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zfAQ1xV-mYh-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pandas import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DOG7vZQutNhv"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "path_to_data = 'BERT-EMBEDDINGS.csv'\n",
    "data_all = read_csv(path_to_data, header=None)  # Skip the first row\n",
    "Y_train = overall_labels\n",
    "Y_train_transposed = np.transpose(Y_train)\n",
    "Y_train = Y_train_transposed[0:2000]\n",
    "X_train = data_all.iloc[0:2000,:].values\n",
    "\n",
    "Y_test = data_all.loc[1:501,0]\n",
    "\n",
    "\n",
    "X_test = data_all.loc[1:501,1:768]\n",
    "\n",
    "\n",
    "dict_classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=10000),\n",
    "    \"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Rbf SVM\": SVC(kernel='rbf', gamma=0.45, C=3.7),\n",
    "    \"Decision Tree\": tree.DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=1000),\n",
    "    \"Neural Net\": MLPClassifier(alpha=1),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(n_estimators=1000),\n",
    "    \"QDA\": QuadraticDiscriminantAnalysis(),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def batch_classify(X_train, Y_train, X_test, Y_test, no_classifiers = 10, verbose = True):\n",
    "    \"\"\"\n",
    "    This method, takes as input the X, Y matrices of the Train and Test set.\n",
    "    And fits them on all of the Classifiers specified in the dict_classifier.\n",
    "    The trained models, and accuracies are saved in a dictionary. The reason to use a dictionary\n",
    "    is because it is very easy to save the whole dictionary with the pickle module.\n",
    "    \n",
    "    Usually, the SVM, Random Forest and Gradient Boosting Classifier take quiet some time to train. \n",
    "    So it is best to train them on a smaller dataset first and \n",
    "    decide whether you want to comment them out or not based on the test accuracy score.\n",
    "    \"\"\"\n",
    "    \n",
    "    dict_models = {}\n",
    "    for classifier_name, classifier in list(dict_classifiers.items())[:no_classifiers]:\n",
    "        t_start = time.process_time()\n",
    "        classifier.fit(X_train, Y_train)\n",
    "        t_end = time.process_time()\n",
    "        \n",
    "        t_diff = t_end - t_start\n",
    "        train_score = classifier.score(X_train, Y_train)\n",
    "        test_score = classifier.score(X_test, Y_test)\n",
    "        \n",
    "        y_pred = classifier.predict(X_test)        \n",
    "        precision_value = metrics.precision_score(Y_test, y_pred)\n",
    "        recall_value = metrics.recall_score(Y_test, y_pred)\n",
    "        f1_value = metrics.f1_score(Y_test, y_pred)\n",
    "        \n",
    "        dict_models[classifier_name] = {'model': classifier, 'train_score': train_score, 'test_score': test_score, 'train_time': t_diff, 'precision':precision_value, 'recall':recall_value, 'f1-value':f1_value}\n",
    "        if verbose:\n",
    "            print(\"trained {c} in {f:.2f} s\".format(c=classifier_name, f=t_diff))\n",
    "    return dict_models\n",
    "\n",
    "\n",
    "\n",
    "def display_dict_models(dict_models, sort_by='test_score'):\n",
    "    cls = [key for key in dict_models.keys()]\n",
    "    test_s = [dict_models[key]['test_score'] for key in cls]\n",
    "    training_s = [dict_models[key]['train_score'] for key in cls]\n",
    "    training_t = [dict_models[key]['train_time'] for key in cls]\n",
    "    prec = [dict_models[key]['precision'] for key in cls]\n",
    "    rec = [dict_models[key]['recall'] for key in cls]\n",
    "    f1 = [dict_models[key]['f1-value'] for key in cls]\n",
    "    \n",
    "    df_ = pd.DataFrame(data=np.zeros(shape=(len(cls),7)), columns = ['classifier', 'train_score', 'test_score', 'train_time','precision','recall','f1-value'])\n",
    "    for ii in range(0,len(cls)):\n",
    "        df_.loc[ii, 'classifier'] = cls[ii]\n",
    "        df_.loc[ii, 'train_score'] = training_s[ii]\n",
    "        df_.loc[ii, 'test_score'] = test_s[ii]\n",
    "        df_.loc[ii, 'train_time'] = training_t[ii]\n",
    "        df_.loc[ii, 'precision'] = prec[ii]\n",
    "        df_.loc[ii, 'recall'] = rec[ii]\n",
    "        df_.loc[ii, 'f1-value'] = f1[ii]\n",
    "    \n",
    "    display(df_.sort_values(by=sort_by, ascending=False))\n",
    "\n",
    "dict_models = batch_classify(X_train, Y_train, X_test, Y_test, no_classifiers = 10)\n",
    "display_dict_models(dict_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kECNbhf3vsDZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
